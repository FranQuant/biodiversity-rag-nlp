{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddafcdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from llama_index.core import VectorStoreIndex, ServiceContext\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core.node_parser import SimpleNodeParser\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.llms import OpenAI\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "import chromadb\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# âœ… Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# âœ… Set API Keys (OpenAI only needed if using OpenAI)\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# âœ… Connect to existing ChromaDB\n",
    "chroma_client = chromadb.PersistentClient(path=\"../data/vector_db\")\n",
    "chroma_collection = chroma_client.get_or_create_collection(\"biodiversity_docs\")\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "\n",
    "# âœ… Choose LLM backend (OpenAI or HuggingFace)\n",
    "USE_OPENAI = True  # Change to False if you want to try HuggingFace\n",
    "\n",
    "if USE_OPENAI:\n",
    "    llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "else:\n",
    "    llm = HuggingFaceLLM(model_name=\"tiiuae/falcon-7b-instruct\", context_window=2048)\n",
    "\n",
    "# âœ… Build service context with selected LLM\n",
    "service_context = ServiceContext.from_defaults(llm=llm)\n",
    "\n",
    "# âœ… Build the index from existing Chroma store\n",
    "index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store, service_context=service_context\n",
    ")\n",
    "\n",
    "# âœ… Set up retriever and query engine\n",
    "retriever = VectorIndexRetriever(index=index, similarity_top_k=3)\n",
    "query_engine = RetrieverQueryEngine.from_args(retriever=retriever)\n",
    "\n",
    "# ðŸ” Ask a question\n",
    "query = \"What are key biodiversity risks investors should monitor?\"\n",
    "response = query_engine.query(query)\n",
    "\n",
    "# ðŸ“˜ Pretty display\n",
    "print(\"\\nâœ¨ Query Completed\")\n",
    "display(Markdown(f\"### ðŸ“˜ Answer:\\n{response.response}\"))\n",
    "\n",
    "print(\"\\nðŸ“Ž Sources:\")\n",
    "for i, node in enumerate(response.source_nodes):\n",
    "    source = node.metadata.get(\"file_name\", \"Unknown\")\n",
    "    preview = node.node.text[:300].strip().replace(\"\\n\", \" \")\n",
    "    display(Markdown(f\"**{i+1}. {source}**\\n> {preview}...\"))\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
