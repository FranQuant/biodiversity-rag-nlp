{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2652 chunked documents.\n",
      "Loaded embedding model: all-MiniLM-L6-v2\n",
      "ChromaDB collection initialized.\n",
      "Indexing document chunks...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b905064df30b4db6bddb155ba9d29632",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/83 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 2652/2652 [00:06<00:00, 416.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed 2652 document chunks.\n",
      "\n",
      "Query: Biodiversity investment risks\n",
      "\n",
      "Match 1: An Investor’s Guide to Biodiversity Risks.pdf | Chunk ID: 128 | Page: 1 | Similarity: 0.7581\n",
      "In addition, several nature-related risks could have reverberating \n",
      "effects, which makes assessing financial risks from nature loss even more difficult. \n",
      "Furthermore, some key data points are rarely disclosed and may be inaccurate when they are. \n",
      "Collecting data for supply chains is an even greater challenge.  \n",
      "However, none of these hurdles are insurmountable and investors may want to integrate biodiversity \n",
      "into their investment decisions for a number of reasons. Investors can take the followi\n",
      "\n",
      "Match 2: JPM_The_Sustainable_Inve_2024-07-11_4700380.pdf | Chunk ID: 500 | Page: 1 | Similarity: 0.7229\n",
      "During this period we have been \n",
      "surprised by the volume of questions from investors on the topic: despite being one of \n",
      "the most challenging themes to invest in (in our opinion), our Biodiversity report \n",
      "remains our most popular global ESG piece. Since its publication, regulation has \n",
      "progressed significantly with biodiversity laws being enacted around the world, and the \n",
      "availability and quality of data and disclosures has improved materially, allowing for a \n",
      "more thorough and reliable screeni\n",
      "\n",
      "Match 3: An Investor’s Guide to Biodiversity Risks.pdf | Chunk ID: 124 | Page: 1 | Similarity: 0.7125\n",
      "13 Global Canopy. 2021. The Little Book of Investing in Nature. https://globalcanopy.org/insights/publication/the-little-book-of-\n",
      "investing-in-nature/. \n",
      "14 WEF. 2020. “Half of World’s GDP Moderately or Highly Dependent on Nature, Says New Report.” \n",
      "https://www.weforum.org/press/2020/01/half-of-world-s-gdp-moderately-or-highly-dependent-on-nature-says-new-report/. \n",
      "\n",
      " \n",
      " \n",
      " MSCI.COM | PAGE 12 OF 28 © 2023 MSCI Inc. All rights reserved. Please refer to the disclaimer at the end of this document. \n",
      " \n",
      "R\n",
      "\n",
      "Query completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 02_build_vector_index.ipynb - Vector Embedding & Indexing Pipeline\n",
    "# --------------------------------------------------\n",
    "# - Loads preprocessed ESG/Biodiversity document chunks\n",
    "# - Computes embeddings using SentenceTransformers\n",
    "# - Stores vectors in ChromaDB (persistent)\n",
    "# - Adds traceable metadata for better RAG citations\n",
    "# - Includes semantic query interface with preview\n",
    "# --------------------------------------------------\n",
    "\n",
    "# 1. IMPORTS & SETUP\n",
    "import os\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from llama_index.core.schema import Document\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# 2. LOAD PRE-PROCESSED CHUNKS\n",
    "chunked_docs_path = Path(\"../outputs/flattened_docs.pkl\")\n",
    "if not chunked_docs_path.exists():\n",
    "    raise FileNotFoundError(f\"Missing preprocessed chunks: {chunked_docs_path}\")\n",
    "\n",
    "with open(chunked_docs_path, \"rb\") as f:\n",
    "    flattened_docs = pickle.load(f)\n",
    "\n",
    "print(f\"Loaded {len(flattened_docs)} chunked documents.\")\n",
    "\n",
    "# 3. INITIALIZE EMBEDDING MODEL\n",
    "embedding_model_name = \"all-MiniLM-L6-v2\"\n",
    "embedding_model = SentenceTransformer(embedding_model_name)\n",
    "print(f\"Loaded embedding model: {embedding_model_name}\")\n",
    "\n",
    "# 4. INITIALIZE VECTOR DATABASE (CHROMADB)\n",
    "vector_db_path = Path(\"../data/vector_db\")\n",
    "vector_db_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "client = chromadb.PersistentClient(path=str(vector_db_path))\n",
    "\n",
    "# Delete existing collection if it exists\n",
    "try:\n",
    "    client.delete_collection(\"biodiversity_docs\")\n",
    "except Exception:\n",
    "    pass  # May not exist yet\n",
    "\n",
    "# Create new collection with metadata\n",
    "collection = client.create_collection(\n",
    "    name=\"biodiversity_docs\",\n",
    "    metadata={\n",
    "        \"hnsw:space\": \"cosine\",\n",
    "        \"embedding_model\": embedding_model_name\n",
    "    }\n",
    ")\n",
    "print(\"ChromaDB collection initialized.\")\n",
    "\n",
    "# 5. COMPUTE EMBEDDINGS & STORE CHUNKS\n",
    "\n",
    "def sanitize_metadata(metadata: dict):\n",
    "    \"\"\"Ensure all metadata values are str, int, float, or bool. Replace None with 'N/A'.\"\"\"\n",
    "    return {\n",
    "        k: (v if isinstance(v, (str, int, float, bool)) else str(v) if v is not None else \"N/A\")\n",
    "        for k, v in metadata.items()\n",
    "    }\n",
    "\n",
    "print(\"Indexing document chunks...\")\n",
    "\n",
    "chunk_texts = [chunk.text for chunk in flattened_docs]\n",
    "chunk_embeddings = embedding_model.encode(chunk_texts, show_progress_bar=True).tolist()\n",
    "\n",
    "for i, chunk in enumerate(tqdm(flattened_docs)):\n",
    "    raw_metadata = {\n",
    "        \"file_name\": chunk.metadata.get(\"file_name\", f\"chunk_{i}.txt\"),\n",
    "        \"chunk_id\": i,\n",
    "        \"source_page\": chunk.metadata.get(\"page_label\", \"N/A\")\n",
    "    }\n",
    "    metadata = sanitize_metadata(raw_metadata)\n",
    "\n",
    "    try:\n",
    "        collection.add(\n",
    "            ids=[f\"chunk_{i}\"],\n",
    "            embeddings=[chunk_embeddings[i]],\n",
    "            metadatas=[metadata],\n",
    "            documents=[chunk.text] if chunk.text else [\"[No content]\"]\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to index chunk {i}: {e}\")\n",
    "\n",
    "print(f\"Indexed {len(flattened_docs)} document chunks.\")\n",
    "\n",
    "# 6. QUERY FUNCTION WITH COSINE SIMILARITY SCORING\n",
    "def query_vector_db(query, top_k=3):\n",
    "    \"\"\"Semantic search over ChromaDB vector index.\"\"\"\n",
    "    query_embedding = embedding_model.encode(query).tolist()\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=top_k,\n",
    "        include=[\"documents\", \"metadatas\", \"distances\"]\n",
    "    )\n",
    "\n",
    "    if not results or not results.get(\"documents\") or not results[\"documents\"][0]:\n",
    "        print(\"No matching documents found.\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    for i, (res, score) in enumerate(zip(results[\"metadatas\"][0], results[\"distances\"][0])):\n",
    "        doc_text = results[\"documents\"][0][i]\n",
    "        preview_text = doc_text[:500] if doc_text else \"[No preview available]\"\n",
    "        similarity = 1 - score  # Convert cosine distance to similarity\n",
    "\n",
    "        print(f\"\\nMatch {i+1}: {res['file_name']} | Chunk ID: {res['chunk_id']} | Page: {res.get('source_page', 'N/A')} | Similarity: {similarity:.4f}\")\n",
    "        print(preview_text)\n",
    "\n",
    "    print(\"\\nQuery completed.\")\n",
    "\n",
    "# 7. TEST QUERY (Optional)\n",
    "if __name__ == \"__main__\":\n",
    "    test_query = \"Biodiversity investment risks\"\n",
    "    query_vector_db(test_query, top_k=3)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
