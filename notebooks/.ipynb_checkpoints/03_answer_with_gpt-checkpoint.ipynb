{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 03_answer_with_gpt.ipynb - ESG Biodiversity RAG Assistant\n",
    "# SECTION 1: Imports & Setup\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import openai\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(dotenv_path=\"/Users/jfsg/biodiversity-rag-nlp/.env\", override=True)\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Load ChromaDB client and collection\n",
    "vector_db_path = Path(\"../data/vector_db\")\n",
    "chroma_client = chromadb.PersistentClient(path=str(vector_db_path))\n",
    "collection = chroma_client.get_collection(name=\"biodiversity_docs\")\n",
    "\n",
    "# Load embedding model\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "core-functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SECTION 2: Core Functions\n",
    "def retrieve_relevant_chunks(query, top_k=5):\n",
    "    query_embedding = embedding_model.encode(query).tolist()\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=top_k,\n",
    "        include=[\"documents\", \"metadatas\", \"distances\"]\n",
    "    )\n",
    "    chunks, citations = [], []\n",
    "    for doc, meta, dist in zip(results[\"documents\"][0], results[\"metadatas\"][0], results[\"distances\"][0]):\n",
    "        similarity = 1 - dist\n",
    "        citation = f\"{meta['file_name']} | Chunk ID: {meta['chunk_id']} | Page: {meta.get('source_page', 'N/A')} | Similarity: {similarity:.4f}\"\n",
    "        chunks.append(doc)\n",
    "        citations.append(citation)\n",
    "    return chunks, citations\n",
    "\n",
    "def compose_prompt(user_query, context_chunks):\n",
    "    system_prompt = (\n",
    "        \"You are a helpful ESG research assistant. Answer using ONLY the provided context.\\n\"\n",
    "        \"Be precise and do not hallucinate. Format the answer in Markdown.\"\n",
    "    )\n",
    "    context_text = \"\\n\\n\".join([f\"Context {i+1}:\\n{chunk}\" for i, chunk in enumerate(context_chunks)])\n",
    "    return f\"{system_prompt}\\n\\n### User Question:\\n{user_query}\\n\\n### Context:\\n{context_text}\\n\\n### Answer:\"\n",
    "\n",
    "def ask_gpt(prompt):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert assistant in ESG and biodiversity investing.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0.3,\n",
    "        max_tokens=1024\n",
    "    )\n",
    "    return response['choices'][0]['message']['content'].strip()\n",
    "\n",
    "def answer_query_with_rag(user_query, top_k=5):\n",
    "    print(\"\\nRetrieving relevant documents...\")\n",
    "    chunks, citations = retrieve_relevant_chunks(user_query, top_k)\n",
    "    print(\"Assembling prompt for GPT...\")\n",
    "    prompt = compose_prompt(user_query, chunks)\n",
    "    print(\"Querying GPT...\")\n",
    "    answer = ask_gpt(prompt)\n",
    "    print(\"\\n--- Final Answer (Markdown Rendered Below) ---\\n\")\n",
    "    print(answer)\n",
    "    print(\"\\n--- Bibliography ---\\n\")\n",
    "    for i, citation in enumerate(citations):\n",
    "        print(f\"[{i+1}] {citation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-modes",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SECTION 3: Choose Execution Mode\n",
    "# A. Single Static Question Mode\n",
    "# user_query = \"What are the financial risks of biodiversity loss for institutional investors?\"\n",
    "# answer_query_with_rag(user_query)\n",
    "\n",
    "# B. Batch Mode - Multiple Questions\n",
    "batch_queries = [\n",
    "    \"What are the financial risks of biodiversity loss for institutional investors?\",\n",
    "    \"What is the role of TNFD in biodiversity risk disclosure?\",\n",
    "    \"How can investors integrate biodiversity KPIs into portfolios?\",\n",
    "    \"Examples of companies with high biodiversity risk exposure\"\n",
    "]\n",
    "for q in batch_queries:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"Question: {q}\")\n",
    "    print(\"=\"*80)\n",
    "    answer_query_with_rag(q)\n",
    "\n",
    "# C. Interactive Mode\n",
    "# while True:\n",
    "#     user_query = input(\"\\nAsk a question (or 'exit' to quit): \")\n",
    "#     if user_query.lower() == 'exit':\n",
    "#         break\n",
    "#     answer_query_with_rag(user_query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}